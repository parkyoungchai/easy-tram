import streamlit as st
import google.generativeai as genai
from PIL import Image
from gtts import gTTS
import speech_recognition as sr
import os
import time

# ğŸ‘‡ 6ë²ˆì§¸ ì¤„: Gemini í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”!
GEMINI_API_KEY = "AIzaSyB-d0aIFMTsQQAsf0_Dm1qupfKOvRsKvo0"

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('models/gemini-flash-latest')

st.set_page_config(page_title="ëŒ€ì „ Easy-Tram", page_icon="ğŸšƒ", layout="centered")
st.title("ğŸšƒ ëŒ€ì „ Easy-Tram")
st.subheader("ì–´ë¥´ì‹ , ê¶ê¸ˆí•œ ê²ƒì„ ì°ì–´ë³´ì„¸ìš”")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "processing" not in st.session_state:
    st.session_state.processing = False

def speak(text):
    try:
        tts = gTTS(text=text, lang='ko')
        filename = f"voice_{int(time.time())}.mp3"
        tts.save(filename)
        st.audio(filename, format="audio/mp3", start_time=0)
    except Exception:
        pass

def listen_to_user():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        status_placeholder = st.empty()
        status_placeholder.toast("ğŸ‘‚ ë§ì”€í•´ì£¼ì„¸ìš”... (ë“£ê³  ìˆì–´ìš”!)")
        try:
            audio = r.listen(source, timeout=5, phrase_time_limit=10)
            status_placeholder.toast("âœ… ì¸ì‹ ì¤‘...")
            text = r.recognize_google(audio, language='ko-KR')
            return text
        except sr.WaitTimeoutError:
             status_placeholder.toast("âš ï¸ ì•„ë¬´ ë§ë„ ì•ˆ ë“¤ë ¸ì–´ìš”.")
             return None
        except sr.UnknownValueError:
             status_placeholder.toast("âš ï¸ ì˜ ëª» ì•Œì•„ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
             return None
        except sr.RequestError:
             status_placeholder.toast("âš ï¸ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
             return None

uploaded_file = st.file_uploader("ì‚¬ì§„ ì°ê¸°", type=["jpg", "png", "jpeg"])

if uploaded_file:
    if st.session_state.uploaded_image != uploaded_file:
        st.session_state.chat_history = []
        st.session_state.uploaded_image = uploaded_file
        st.session_state.processing = False # ìƒˆ ì´ë¯¸ì§€ ì˜¬ë¦¬ë©´ ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”

    image = Image.open(uploaded_file)
    st.image(image, caption='ì°ì€ ì‚¬ì§„')

    # --- [1ì°¨ ë¶„ì„] ---
    if not st.session_state.chat_history and not st.session_state.processing:
        st.session_state.processing = True # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        with st.spinner('AI ë¹„ì„œê°€ ì‚¬ì§„ì„ ë³´ê³  ìˆìŠµë‹ˆë‹¤...'):
            try:
                prompt = """
                ë‹¹ì‹ ì€ ì–´ë¥´ì‹ ì„ ìœ„í•œ 'êµí†µ ì•ˆë‚´ ë¹„ì„œ'ì…ë‹ˆë‹¤.
                ì‚¬ì§„ì„ ë³´ê³  ì–´ë¥´ì‹ ì´ ê¼­ ì•„ì…”ì•¼ í•  í•µì‹¬ ë‚´ìš©ë§Œ ì‰¬ìš´ ìš°ë¦¬ë§ ì¡´ëŒ“ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                3~5ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•˜ê³ , "ì–´ë¥´ì‹ ," í•˜ê³  ë¶€ë¥´ë©° ì‹œì‘í•˜ì„¸ìš”.
                (ì ˆëŒ€ë¡œ ì˜ì–´ë¡œ ëŒ€ë‹µí•˜ì§€ ë§ˆì„¸ìš”.)
                """
                response = model.generate_content([prompt, image])
                st.session_state.chat_history.append({"role": "ai", "text": response.text})
                st.session_state.processing = False # ì²˜ë¦¬ ì™„ë£Œ
                st.rerun()
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
                st.session_state.processing = False

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for i, message in enumerate(st.session_state.chat_history):
        if message["role"] == "ai":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.write(message['text'])
                if i == len(st.session_state.chat_history) - 1:
                     speak(message['text'])
        else:
             with st.chat_message("user", avatar="ğŸ‘¤"):
                st.write(message['text'])

    # --- [ë¬´í•œ ì§ˆë¬¸ ê¸°ëŠ¥ (í•­ìƒ ë– ìˆìŒ)] ---
    st.write("---")
    
    # 1. ìŒì„± ì…ë ¥ ë²„íŠ¼ (í•­ìƒ ìœ„ì—)
    if st.button("ğŸ¤ ëˆŒëŸ¬ì„œ ë§í•˜ê¸°", use_container_width=True):
        voice_text = listen_to_user()
        if voice_text:
            st.session_state.chat_history.append({"role": "user", "text": voice_text})
            st.rerun()

    # 2. í…ìŠ¤íŠ¸ ì…ë ¥ì°½ (í•­ìƒ ì•„ë˜ì—)
    # 'st.chat_input'ì„ ì“°ë©´ ì—”í„°í‚¤ ì²˜ë¦¬ê°€ í›¨ì”¬ ë§¤ë„ëŸ½ìŠµë‹ˆë‹¤!
    if user_input := st.chat_input("ë” ê¶ê¸ˆí•œ ì ì„ ì—¬ê¸°ì— ì ì–´ì£¼ì„¸ìš”"):
         st.session_state.chat_history.append({"role": "user", "text": user_input})
         st.rerun()

    # --- [AI ë‹µë³€ ìƒì„± ë¡œì§] ---
    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ 'ìœ ì €'ì˜ ì§ˆë¬¸ì´ë©´ AIê°€ ëŒ€ë‹µí•  ì°¨ë¡€!
    if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
        with st.spinner('ë‹µë³€ì„ ìƒê° ì¤‘ì…ë‹ˆë‹¤...'):
            try:
                last_question = st.session_state.chat_history[-1]["text"]
                
                # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì–´ëŠ ì •ë„ ê¸°ì–µí•˜ê²Œ ë§Œë“¤ë©´ ë” ë˜‘ë˜‘í•´ì§‘ë‹ˆë‹¤.
                history_text = "\n".join([f"{msg['role']}: {msg['text']}" for msg in st.session_state.chat_history[-5:]])
                
                follow_up_prompt = f"""
                [ì´ì „ ëŒ€í™” ê¸°ë¡]
                {history_text}
                
                [ìƒˆë¡œìš´ ì§ˆë¬¸]
                ì–´ë¥´ì‹ : {last_question}
                
                ìœ„ ëŒ€í™” íë¦„ì„ ë³´ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•´ ì‰½ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. (í•œêµ­ì–´ë§Œ ì‚¬ìš©)
                """
                response = model.generate_content([follow_up_prompt, image])
                st.session_state.chat_history.append({"role": "ai", "text": response.text})
                st.rerun()
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")